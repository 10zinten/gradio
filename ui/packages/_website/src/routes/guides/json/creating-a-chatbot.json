{"guide": {"name": "creating-a-chatbot", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 27, "pretty_name": "Creating A Chatbot", "content": "# How to Create a Chatbot\n\nRelated spaces: https://huggingface.co/spaces/dawood/chatbot-guide, https://huggingface.co/spaces/dawood/chatbot-guide-multimodal, https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B, https://huggingface.co/spaces/gorkemgoknar/moviechatbot, https://huggingface.co/spaces/Kirili4ik/chat-with-Kirill\nTags: NLP, TEXT, HTML\n\n## Introduction\n\nChatbots are widely studied in natural language processing (NLP) research and are a common use case of NLP in industry. Because chatbots are designed to be used directly by customers and end users, it is important to validate that chatbots are behaving as expected when confronted with a wide variety of input prompts. \n\nUsing `gradio`, you can easily build a demo of your chatbot model and share that with a testing team, or test it yourself using an intuitive chatbot GUI.\n\nThis tutorial will show how to take a pretrained chatbot model and deploy it with a Gradio interface in 4 steps. The live chatbot interface that we create will look something like this (try it!):\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nChatbots are *stateful*, meaning that the model's prediction can change depending on how the user has previously interacted with the model. So, in this tutorial, we will also cover how to use **state** with Gradio demos. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/quickstart). To use a pretrained chatbot model, also install `transformers` and `torch`. \n\nLet's get started! Here's how to build your own chatbot: \n\n  [1. Set up the Chatbot Model](#1-set-up-the-chatbot-model)\n  [2. Define a `predict` function](#2-define-a-predict-function)\n  [3. Create a Gradio Demo using Blocks](#3-create-a-gradio-demo-using-blocks)\n  [4. Chatbot Markdown Support](#4-chatbot-markdown-support)\n\n## 1. Set up the Chatbot Model\n\nFirst, you will need to have a chatbot model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will use a pretrained chatbot model, `DialoGPT`, and its tokenizer from the [Hugging Face Hub](https://huggingface.co/microsoft/DialoGPT-medium), but you can replace this with your own model. \n\nHere is the code to load `DialoGPT` from Hugging Face `transformers`.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n```\n\n## 2. Define a `predict` function\n\nNext, you will need to define a function that takes in the *user input* as well as the previous *chat history* to generate a response.\n\nIn the case of our pretrained model, it will look like this:\n\n```python\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n```\n\nLet's break this down. The function takes two parameters:\n\n* `input`: which is what the user enters (through the Gradio GUI) in a particular step of the conversation. \n* `history`: which represents the **state**, consisting of the list of user and bot responses. To create a stateful Gradio demo, we *must* pass in a parameter to represent the state, and we set the default value of this parameter to be the initial value of the state (in this case, the empty list since this is what we would like the chat history to be at the start).\n\nThen, the function tokenizes the input and concatenates it with the tokens corresponding to the previous user and bot responses. Then, this is fed into the pretrained model to get a prediction. Finally, we do some cleaning up so that we can return two values from our function:\n\n* `response`: which is a list of tuples of strings corresponding to all of the user and bot responses. This will be rendered as the output in the Gradio demo.\n* `history` variable, which is the token representation of all of the user and bot responses. In stateful Gradio demos, we *must* return the updated state at the end of the function. \n\n## 3. Create a Gradio Demo using Blocks\n\nNow that we have our predictive function set up, we can create a Gradio demo around it. \n\nIn this case, our function takes in two values, a text input and a state input. The corresponding input components in `gradio` are `\"text\"` and `\"state\"`. \n\nThe function also returns two values. We will display the list of responses using the dedicated `\"chatbot\"` component and use the `\"state\"` output component type for the second return value.\n\nNote that the `\"state\"` input and output components are not displayed. \n\n```python\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n    \n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n            \n    txt.submit(predict, [txt, state], [chatbot, state])\n            \ndemo.launch()\n```\n\nThis produces the following demo, which you can try right here in your browser (try typing in some simple greetings like \"Hi!\" to get started):\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n----------\n\n## 4. Chatbot Markdown Support\n\n\nThe `gr.Chatbot` also supports a subset of markdown including bold, italics, code, and images. Let's take a look at how we can use the markdown support to allow a user to submit images to the chatbot component.\n\n```python\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n```\n\n\nNotice the `add_image` function takes in both the `state` and `image` and appends the user submitted image to `state` by using markdown. \n\n\n```python\nimport gradio as gr\n\ndef add_text(state, text):\n    state = state + [(text, text + \"?\")]\n    return state, state\n\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n\n\nwith gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n    state = gr.State([])\n    \n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.UploadButton(\"\ud83d\uddbc\ufe0f\", file_types=[\"image\"])\n            \n    txt.submit(add_text, [state, txt], [state, chatbot])\n    txt.submit(lambda :\"\", None, txt)\n    btn.upload(add_image, [state, btn], [state, chatbot])\n            \ndemo.launch()\n```\n\nThis is the code for a chatbot with a textbox for a user to submit text and an image upload button to submit images. The rest of the demo code is creating an interface using blocks; basically adding a couple more components compared to section 3.\n\nThis code will produce a demo like the one below:\n\n<iframe src=\"https://dawood-chatbot-guide-multimodal.hf.space\" frameBorder=\"0\" height=\"650\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nAnd you're done! That's all the code you need to build an interface for your chatbot model. Here are some references that you may find useful:\n\n* Gradio's [Quickstart guide](https://gradio.app/quickstart/)\n* The first chatbot demo [chatbot demo](https://huggingface.co/spaces/dawood/chatbot-guide) and [complete code](https://huggingface.co/spaces/dawood/chatbot-guide/blob/main/app.py) (on Hugging Face Spaces)\n* The final chatbot with markdown support [chatbot demo](https://huggingface.co/spaces/dawood/chatbot-guide-multimodal) and [complete code](https://huggingface.co/spaces/dawood/chatbot-guide-multimodal/blob/main/app.py) (on Hugging Face Spaces)\n", "html": "<h1 id=\"how-to-create-a-chatbot\">How to Create a Chatbot</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Chatbots are widely studied in natural language processing (NLP) research and are a common use case of NLP in industry. Because chatbots are designed to be used directly by customers and end users, it is important to validate that chatbots are behaving as expected when confronted with a wide variety of input prompts. </p>\n\n<p>Using <code>gradio</code>, you can easily build a demo of your chatbot model and share that with a testing team, or test it yourself using an intuitive chatbot GUI.</p>\n\n<p>This tutorial will show how to take a pretrained chatbot model and deploy it with a Gradio interface in 4 steps. The live chatbot interface that we create will look something like this (try it!):</p>\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Chatbots are <em>stateful</em>, meaning that the model's prediction can change depending on how the user has previously interacted with the model. So, in this tutorial, we will also cover how to use <strong>state</strong> with Gradio demos. </p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/quickstart\">installed</a>. To use a pretrained chatbot model, also install <code>transformers</code> and <code>torch</code>. </p>\n\n<p>Let's get started! Here's how to build your own chatbot: </p>\n\n<p><a href=\"#1-set-up-the-chatbot-model\">1. Set up the Chatbot Model</a>\n  <a href=\"#2-define-a-predict-function\">2. Define a <code>predict</code> function</a>\n  <a href=\"#3-create-a-gradio-demo-using-blocks\">3. Create a Gradio Demo using Blocks</a>\n  <a href=\"#4-chatbot-markdown-support\">4. Chatbot Markdown Support</a></p>\n\n<h2 id=\"1-set-up-the-chatbot-model\">1. Set up the Chatbot Model</h2>\n\n<p>First, you will need to have a chatbot model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will use a pretrained chatbot model, <code>DialoGPT</code>, and its tokenizer from the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/microsoft/DialoGPT-medium\">Hugging Face Hub</a>, but you can replace this with your own model. </p>\n\n<p>Here is the code to load <code>DialoGPT</code> from Hugging Face <code>transformers</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n</code></pre></div>\n\n<h2 id=\"2-define-a-predict-function\">2. Define a <code>predict</code> function</h2>\n\n<p>Next, you will need to define a function that takes in the <em>user input</em> as well as the previous <em>chat history</em> to generate a response.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response \n    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n    return response, history\n</code></pre></div>\n\n<p>Let's break this down. The function takes two parameters:</p>\n\n<ul>\n<li><code>input</code>: which is what the user enters (through the Gradio GUI) in a particular step of the conversation. </li>\n<li><code>history</code>: which represents the <strong>state</strong>, consisting of the list of user and bot responses. To create a stateful Gradio demo, we <em>must</em> pass in a parameter to represent the state, and we set the default value of this parameter to be the initial value of the state (in this case, the empty list since this is what we would like the chat history to be at the start).</li>\n</ul>\n\n<p>Then, the function tokenizes the input and concatenates it with the tokens corresponding to the previous user and bot responses. Then, this is fed into the pretrained model to get a prediction. Finally, we do some cleaning up so that we can return two values from our function:</p>\n\n<ul>\n<li><code>response</code>: which is a list of tuples of strings corresponding to all of the user and bot responses. This will be rendered as the output in the Gradio demo.</li>\n<li><code>history</code> variable, which is the token representation of all of the user and bot responses. In stateful Gradio demos, we <em>must</em> return the updated state at the end of the function. </li>\n</ul>\n\n<h2 id=\"3-create-a-gradio-demo-using-blocks\">3. Create a Gradio Demo using Blocks</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio demo around it. </p>\n\n<p>In this case, our function takes in two values, a text input and a state input. The corresponding input components in <code>gradio</code> are <code>\"text\"</code> and <code>\"state\"</code>. </p>\n\n<p>The function also returns two values. We will display the list of responses using the dedicated <code>\"chatbot\"</code> component and use the <code>\"state\"</code> output component type for the second return value.</p>\n\n<p>Note that the <code>\"state\"</code> input and output components are not displayed. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    state = gr.State([])\n\n    with gr.Row():\n        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n\n    txt.submit(predict, [txt, state], [chatbot, state])\n\ndemo.launch()\n</code></pre></div>\n\n<p>This produces the following demo, which you can try right here in your browser (try typing in some simple greetings like \"Hi!\" to get started):</p>\n\n<iframe src=\"https://dawood-chatbot-guide.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<h2 id=\"4-chatbot-markdown-support\">4. Chatbot Markdown Support</h2>\n\n<p>The <code>gr.Chatbot</code> also supports a subset of markdown including bold, italics, code, and images. Let's take a look at how we can use the markdown support to allow a user to submit images to the chatbot component.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n</code></pre></div>\n\n<p>Notice the <code>add_image</code> function takes in both the <code>state</code> and <code>image</code> and appends the user submitted image to <code>state</code> by using markdown. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ndef add_text(state, text):\n    state = state + [(text, text + \"?\")]\n    return state, state\n\ndef add_image(state, image):\n    state = state + [(f\"![](/file={image.name})\", \"Cool pic!\")]\n    return state, state\n\n\nwith gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n    state = gr.State([])\n\n    with gr.Row():\n        with gr.Column(scale=0.85):\n            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n        with gr.Column(scale=0.15, min_width=0):\n            btn = gr.UploadButton(\"\ud83d\uddbc\ufe0f\", file_types=[\"image\"])\n\n    txt.submit(add_text, [state, txt], [state, chatbot])\n    txt.submit(lambda :\"\", None, txt)\n    btn.upload(add_image, [state, btn], [state, chatbot])\n\ndemo.launch()\n</code></pre></div>\n\n<p>This is the code for a chatbot with a textbox for a user to submit text and an image upload button to submit images. The rest of the demo code is creating an interface using blocks; basically adding a couple more components compared to section 3.</p>\n\n<p>This code will produce a demo like the one below:</p>\n\n<iframe src=\"https://dawood-chatbot-guide-multimodal.hf.space\" frameBorder=\"0\" height=\"650\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>And you're done! That's all the code you need to build an interface for your chatbot model. Here are some references that you may find useful:</p>\n\n<ul>\n<li>Gradio's <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/quickstart/\">Quickstart guide</a></li>\n<li>The first chatbot demo <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide\">chatbot demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide/blob/main/app.py\">complete code</a> (on Hugging Face Spaces)</li>\n<li>The final chatbot with markdown support <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide-multimodal\">chatbot demo</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/dawood/chatbot-guide-multimodal/blob/main/app.py\">complete code</a> (on Hugging Face Spaces)</li>\n</ul>\n", "tags": ["NLP", "TEXT", "HTML"], "spaces": ["https://huggingface.co/spaces/dawood/chatbot-guide", "https://huggingface.co/spaces/dawood/chatbot-guide-multimodal", "https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B", "https://huggingface.co/spaces/gorkemgoknar/moviechatbot", "https://huggingface.co/spaces/Kirili4ik/chat-with-Kirill"], "url": "/guides/creating-a-chatbot/", "contributor": null}}