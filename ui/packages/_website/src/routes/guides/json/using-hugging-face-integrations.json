{"guide": {"name": "using-hugging-face-integrations", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": 1, "absolute_index": 13, "pretty_name": "Using Hugging Face Integrations", "content": "# Using Hugging Face Integrations\n\nRelated spaces: https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder, https://huggingface.co/spaces/osanseviero/helsinki_translation_en_es, https://huggingface.co/spaces/osanseviero/remove-bg-webcam, https://huggingface.co/spaces/mrm8488/GPT-J-6B, https://huggingface.co/spaces/akhaliq/T0pp, https://huggingface.co/spaces/osanseviero/mix_match_gradio\nTags: HUB, SPACES, EMBED\n\nContributed by <a href=\"https://huggingface.co/osanseviero\">Omar Sanseviero</a> \ud83e\udd99 and <a href=\"https://huggingface.co/farukozderim\">\u00d6mer Faruk \u00d6zdemir</a>\n\n## Introduction\n\nThe Hugging Face Hub is a central platform that has over 90,000 [models](https://huggingface.co/models), 14,000 [datasets](https://huggingface.co/datasets) and 14,000 [demos](https://huggingface.co/spaces), also known as Spaces. From Natural Language Processing to Computer Vision and Speech, the Hub supports multiple domains. Although Hugging Face is famous for its \ud83e\udd17 transformers and diffusers libraries, the Hub also supports dozens of ML libraries, such as PyTorch, TensorFlow, spaCy, and many others.\n\nGradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.\n\n## Using regular inference with `pipeline`\n\nFirst, let's build a simple interface that translates text from English to Spanish. Between the over a thousand models shared by the University of Helsinki, there is an [existing model](https://huggingface.co/Helsinki-NLP/opus-mt-en-es), `opus-mt-en-es`, that does precisely this!\n\nThe \ud83e\udd17 transformers library has a very easy-to-use abstraction, [`pipeline()`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline) that handles most of the complex code to offer a simple API for common tasks. By specifying the task and an (optional) model, you can use an existing model with few lines:\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndef predict(text):\n  return pipe(text)[0][\"translation_text\"]\n  \niface = gr.Interface(\n  fn=predict, \n  inputs='text',\n  outputs='text',\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n```\n\nThe previous code produces the following interface, which you can try right here in your browser: \n\n<iframe src=\"https://osanseviero-helsinki-translation-en-es.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nThis demo requires installing four libraries: gradio, torch, transformers, and sentencepiece. Apart from that, this is a Gradio with the structure you're used to! The demo is a usual Gradio `Interface` with a prediction function, a specified input, and a specified output. The prediction function executes the `pipeline` function with the given input, retrieves the first (and only) translation result, and returns the `translation_text` field, which you're interested in.\n\n## Using Hugging Face Inference API\n\nHugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production.\n\nLet's try the same demo as above but using the Inference API instead of loading the model yourself. Given a Hugging Face model supported in the Inference API, Gradio can automatically infer the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function. Here is what the code would look like!\n\n```python\nimport gradio as gr\n\niface = gr.Interface.load(\"huggingface/Helsinki-NLP/opus-mt-en-es\",\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n```\n\nLet's go over some of the key differences:\n\n* `Interface.load()` is used instead of the usual `Interface()`.\n* `Interface.load()` receives a string with the prefix `huggingface/`, and then the model repository ID.\n* Since the input, output and prediction functions are not needed, you only need to modify the UI parts (such as `title`, `description`, and `examples`).\n* There is no need to install any dependencies (except Gradio) since you are not loading the model on your computer.\n\nYou might notice that the first inference takes about 20 seconds. This happens since the Inference API is loading the model in the server. You get some benefits afterward:\n\n* The inference will be much faster.\n* The server caches your requests.\n* You get built-in automatic scaling.\n\n## Hosting your Gradio demos\n\n\n[Hugging Face Spaces](https://hf.co/spaces) allows anyone to host their Gradio demos freely. The community shares oven 2,000 Spaces. Uploading your Gradio demos take a couple of minutes. You can head to [hf.co/new-space](https://huggingface.co/new-space), select the Gradio SDK, create an `app.py` file, and voila! You have a demo you can share with anyone else.\n\n## Building demos based on other demos\n\nYou can use the existing Spaces to tweak the UI or combine multiple demos. Let's find how to do this! First, let's take a look at an existing demo that does background removal. \n\nThis is a Gradio demo [already shared](https://huggingface.co/spaces/eugenesiow/remove-bg) by a community member. You can load an existing demo using `Interface` in a syntax similar to how it's done for the Inference API. It just takes two lines of code and with the prefix `spaces`.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\"spaces/eugenesiow/remove-bg\").launch()\n```\n\nThe code snippet above will load the same interface as the corresponding Space demo.\n\n<iframe src=\"https://eugenesiow-remove-bg.hf.space\" frameBorder=\"0\" height=\"900\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nYou can change UI elements, such as the title or theme, but also change the expected type. The previous Space expected users to upload images. What if you would like users to have their webcam and remove the background from there? You can load the Space but change the source of input as follows:\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n  \"spaces/eugenesiow/remove-bg\", \n  inputs=[gr.Image(label=\"Input Image\", source=\"webcam\")]\n).launch()\n```\n\nThe code above generates the following demo.\n\n<iframe src=\"https://osanseviero-remove-bg-webcam.hf.space\" frameBorder=\"0\" height=\"600\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nAs you can see, the demo looks the same, but it uses a webcam input instead of user-uploaded images.\n\nYou can learn more about this feature, and how to use it with the new Blocks API in the [Using Gradio Blocks Like Functions guide](/using_blocks_like_functions)\n\n## Using multiple Spaces\n\nSometimes a single model inference will not be enough: you might want to call multiple models by piping them (using the output of model A as the input of model B). `Series` can achieve this. Other times, you might want to run two models in parallel to compare them. `Parallel` can do this!\n\nLet's combine the notion of running things in parallel with the Spaces integration. The [GPT-J-6B](https://huggingface.co/spaces/mrm8488/GPT-J-6B) Space demos a model that generates text using a model called GPT-J. The [T0pp](https://huggingface.co/spaces/akhaliq/T0pp) Space demos another generative model called T0pp. Let's see how to combine both into one.\n\n```python\nimport gradio as gr\n\niface1 = gr.Interface.load(\"spaces/mrm8488/GPT-J-6B\")\niface2 = gr.Interface.load(\"spaces/akhaliq/T0pp\")\n\niface3 = gr.mix.Parallel(\n  iface1, iface2, \n  examples = [\n    ['Which country will win the 2002 World Cup?'],\n    [\"A is the son's of B's uncle. What is the family relationship between A and B?\"],\n    [\"In 2030, \"],\n  ])\n  \niface3.launch()\n```\n\n`iface1` and `iface2` are loading existing Spaces. Then, with `Parallel`, you can run the interfaces parallelly. When you click submit, you will get the output for both interfaces. This is how the demo looks like:\n\n<iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nAlthough both models are generative, you can see that the way both models behave is very different. That's a powerful application of `Parallel`!\n\n## Creating Spaces with python\n\nMaking use of the [huggingface_hub client library](https://huggingface.co/docs/huggingface_hub/index) library you can create new Spaces or model repositories. You can do this even in a Gradio Space! You can find an example space [here](https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder). This Space creates a new Space comparing different models or spaces with the support of Gradio `load` and `Parallel`. Now you can try creating cool spaces with all kinds of functionality \ud83d\ude0e.\n\n```python\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file,\n)\ncreate_repo(name=target_space_name, token=hf_token, repo_type=\"space\", space_sdk=\"gradio\")\nrepo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)\nfile_url = upload_file(\n    path_or_fileobj=\"file.txt\",\n    path_in_repo=\"app.py\",\n    repo_id=repo_name,\n    repo_type=\"space\",\n    token=hf_token,\n)\n```\nHere, `create_repo` creates a gradio repo with the target name under a specific account using that account's Write Token. `repo_name` gets the full repo name of the related repo. Finally `upload_file` uploads a file inside the repo with the name `app.py`.\n\n<iframe src=\"https://farukozderim-model-comparator-space-builder.hf.space\" frameBorder=\"0\" height=\"800\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\n## Embedding your Space demo on other websites\n\nThroughout this guide, you've seen there are Gradio demos embedded. You can also do this on own website! The first step is to create a Space with the demo you want to showcase. You can embed it in your HTML code, as shown in the following self-contained example.\n\n```bash\n&lt;iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"&gt;&lt;/iframe&gt;\n```\n\n## Recap\n\nThat's it! Let's recap what you can do:\n\n1. Host your Gradio demos in Spaces.\n2. Use the Inference API to build demos in two lines of code.\n3. Load existing Spaces and modify them.\n4. Combine multiple Spaces by running them sequentially or parallelly.\n5. Embed your Space demo directly on a website.\n\n\ud83e\udd17\n", "html": "<h1 id=\"using-hugging-face-integrations\">Using Hugging Face Integrations</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>The Hugging Face Hub is a central platform that has over 90,000 <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models\">models</a>, 14,000 <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/datasets\">datasets</a> and 14,000 <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces\">demos</a>, also known as Spaces. From Natural Language Processing to Computer Vision and Speech, the Hub supports multiple domains. Although Hugging Face is famous for its \ud83e\udd17 transformers and diffusers libraries, the Hub also supports dozens of ML libraries, such as PyTorch, TensorFlow, spaCy, and many others.</p>\n\n<p>Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.</p>\n\n<h2 id=\"using-regular-inference-with-pipeline\">Using regular inference with <code>pipeline</code></h2>\n\n<p>First, let's build a simple interface that translates text from English to Spanish. Between the over a thousand models shared by the University of Helsinki, there is an <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/Helsinki-NLP/opus-mt-en-es\">existing model</a>, <code>opus-mt-en-es</code>, that does precisely this!</p>\n\n<p>The \ud83e\udd17 transformers library has a very easy-to-use abstraction, <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline\"><code>pipeline()</code></a> that handles most of the complex code to offer a simple API for common tasks. By specifying the task and an (optional) model, you can use an existing model with few lines:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndef predict(text):\n  return pipe(text)[0][\"translation_text\"]\n\niface = gr.Interface(\n  fn=predict, \n  inputs='text',\n  outputs='text',\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n</code></pre></div>\n\n<p>The previous code produces the following interface, which you can try right here in your browser: </p>\n\n<iframe src=\"https://osanseviero-helsinki-translation-en-es.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>This demo requires installing four libraries: gradio, torch, transformers, and sentencepiece. Apart from that, this is a Gradio with the structure you're used to! The demo is a usual Gradio <code>Interface</code> with a prediction function, a specified input, and a specified output. The prediction function executes the <code>pipeline</code> function with the given input, retrieves the first (and only) translation result, and returns the <code>translation_text</code> field, which you're interested in.</p>\n\n<h2 id=\"using-hugging-face-inference-api\">Using Hugging Face Inference API</h2>\n\n<p>Hugging Face has a free service called the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/inference-api\">Inference API</a>, which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/pricing\">Inference Endpoints</a> when you want to use it in production.</p>\n\n<p>Let's try the same demo as above but using the Inference API instead of loading the model yourself. Given a Hugging Face model supported in the Inference API, Gradio can automatically infer the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function. Here is what the code would look like!</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\niface = gr.Interface.load(\"huggingface/Helsinki-NLP/opus-mt-en-es\",\n  examples=[[\"Hello! My name is Omar\"]]\n)\n\niface.launch()\n</code></pre></div>\n\n<p>Let's go over some of the key differences:</p>\n\n<ul>\n<li><code>Interface.load()</code> is used instead of the usual <code>Interface()</code>.</li>\n<li><code>Interface.load()</code> receives a string with the prefix <code>huggingface/</code>, and then the model repository ID.</li>\n<li>Since the input, output and prediction functions are not needed, you only need to modify the UI parts (such as <code>title</code>, <code>description</code>, and <code>examples</code>).</li>\n<li>There is no need to install any dependencies (except Gradio) since you are not loading the model on your computer.</li>\n</ul>\n\n<p>You might notice that the first inference takes about 20 seconds. This happens since the Inference API is loading the model in the server. You get some benefits afterward:</p>\n\n<ul>\n<li>The inference will be much faster.</li>\n<li>The server caches your requests.</li>\n<li>You get built-in automatic scaling.</li>\n</ul>\n\n<h2 id=\"hosting-your-gradio-demos\">Hosting your Gradio demos</h2>\n\n<p><a rel=\"noopener\" target=\"_blank\" href=\"https://hf.co/spaces\">Hugging Face Spaces</a> allows anyone to host their Gradio demos freely. The community shares oven 2,000 Spaces. Uploading your Gradio demos take a couple of minutes. You can head to <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/new-space\">hf.co/new-space</a>, select the Gradio SDK, create an <code>app.py</code> file, and voila! You have a demo you can share with anyone else.</p>\n\n<h2 id=\"building-demos-based-on-other-demos\">Building demos based on other demos</h2>\n\n<p>You can use the existing Spaces to tweak the UI or combine multiple demos. Let's find how to do this! First, let's take a look at an existing demo that does background removal. </p>\n\n<p>This is a Gradio demo <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/eugenesiow/remove-bg\">already shared</a> by a community member. You can load an existing demo using <code>Interface</code> in a syntax similar to how it's done for the Inference API. It just takes two lines of code and with the prefix <code>spaces</code>.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface.load(\"spaces/eugenesiow/remove-bg\").launch()\n</code></pre></div>\n\n<p>The code snippet above will load the same interface as the corresponding Space demo.</p>\n\n<iframe src=\"https://eugenesiow-remove-bg.hf.space\" frameBorder=\"0\" height=\"900\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>You can change UI elements, such as the title or theme, but also change the expected type. The previous Space expected users to upload images. What if you would like users to have their webcam and remove the background from there? You can load the Space but change the source of input as follows:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface.load(\n  \"spaces/eugenesiow/remove-bg\", \n  inputs=[gr.Image(label=\"Input Image\", source=\"webcam\")]\n).launch()\n</code></pre></div>\n\n<p>The code above generates the following demo.</p>\n\n<iframe src=\"https://osanseviero-remove-bg-webcam.hf.space\" frameBorder=\"0\" height=\"600\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>As you can see, the demo looks the same, but it uses a webcam input instead of user-uploaded images.</p>\n\n<p>You can learn more about this feature, and how to use it with the new Blocks API in the <a rel=\"noopener\" target=\"_blank\" href=\"/using_blocks_like_functions\">Using Gradio Blocks Like Functions guide</a></p>\n\n<h2 id=\"using-multiple-spaces\">Using multiple Spaces</h2>\n\n<p>Sometimes a single model inference will not be enough: you might want to call multiple models by piping them (using the output of model A as the input of model B). <code>Series</code> can achieve this. Other times, you might want to run two models in parallel to compare them. <code>Parallel</code> can do this!</p>\n\n<p>Let's combine the notion of running things in parallel with the Spaces integration. The <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/mrm8488/GPT-J-6B\">GPT-J-6B</a> Space demos a model that generates text using a model called GPT-J. The <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/akhaliq/T0pp\">T0pp</a> Space demos another generative model called T0pp. Let's see how to combine both into one.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\niface1 = gr.Interface.load(\"spaces/mrm8488/GPT-J-6B\")\niface2 = gr.Interface.load(\"spaces/akhaliq/T0pp\")\n\niface3 = gr.mix.Parallel(\n  iface1, iface2, \n  examples = [\n    ['Which country will win the 2002 World Cup?'],\n    [\"A is the son's of B's uncle. What is the family relationship between A and B?\"],\n    [\"In 2030, \"],\n  ])\n\niface3.launch()\n</code></pre></div>\n\n<p><code>iface1</code> and <code>iface2</code> are loading existing Spaces. Then, with <code>Parallel</code>, you can run the interfaces parallelly. When you click submit, you will get the output for both interfaces. This is how the demo looks like:</p>\n\n<iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Although both models are generative, you can see that the way both models behave is very different. That's a powerful application of <code>Parallel</code>!</p>\n\n<h2 id=\"creating-spaces-with-python\">Creating Spaces with python</h2>\n\n<p>Making use of the <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/docs/huggingface_hub/index\">huggingface_hub client library</a> library you can create new Spaces or model repositories. You can do this even in a Gradio Space! You can find an example space <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder\">here</a>. This Space creates a new Space comparing different models or spaces with the support of Gradio <code>load</code> and <code>Parallel</code>. Now you can try creating cool spaces with all kinds of functionality \ud83d\ude0e.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file,\n)\ncreate_repo(name=target_space_name, token=hf_token, repo_type=\"space\", space_sdk=\"gradio\")\nrepo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)\nfile_url = upload_file(\n    path_or_fileobj=\"file.txt\",\n    path_in_repo=\"app.py\",\n    repo_id=repo_name,\n    repo_type=\"space\",\n    token=hf_token,\n)\n</code></pre></div>\n\n<p>Here, <code>create_repo</code> creates a gradio repo with the target name under a specific account using that account's Write Token. <code>repo_name</code> gets the full repo name of the related repo. Finally <code>upload_file</code> uploads a file inside the repo with the name <code>app.py</code>.</p>\n\n<iframe src=\"https://farukozderim-model-comparator-space-builder.hf.space\" frameBorder=\"0\" height=\"800\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"embedding-your-space-demo-on-other-websites\">Embedding your Space demo on other websites</h2>\n\n<p>Throughout this guide, you've seen there are Gradio demos embedded. You can also do this on own website! The first step is to create a Space with the demo you want to showcase. You can embed it in your HTML code, as shown in the following self-contained example.</p>\n\n<div class='codeblock'><pre><code class='lang-bash'>&lt;iframe src=\"https://osanseviero-mix-match-gradio.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"&gt;&lt;/iframe&gt;\n</code></pre></div>\n\n<h2 id=\"recap\">Recap</h2>\n\n<p>That's it! Let's recap what you can do:</p>\n\n<ol>\n<li>Host your Gradio demos in Spaces.</li>\n<li>Use the Inference API to build demos in two lines of code.</li>\n<li>Load existing Spaces and modify them.</li>\n<li>Combine multiple Spaces by running them sequentially or parallelly.</li>\n<li>Embed your Space demo directly on a website.</li>\n</ol>\n\n<p>\ud83e\udd17</p>\n", "tags": ["HUB", "SPACES", "EMBED"], "spaces": ["https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder", "https://huggingface.co/spaces/osanseviero/helsinki_translation_en_es", "https://huggingface.co/spaces/osanseviero/remove-bg-webcam", "https://huggingface.co/spaces/mrm8488/GPT-J-6B", "https://huggingface.co/spaces/akhaliq/T0pp", "https://huggingface.co/spaces/osanseviero/mix_match_gradio"], "url": "/guides/using-hugging-face-integrations/", "contributor": "<a href=\"https://huggingface.co/osanseviero\">Omar Sanseviero</a> \ud83e\udd99 and <a href=\"https://huggingface.co/farukozderim\">\u00d6mer Faruk \u00d6zdemir</a>"}}